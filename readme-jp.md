# AIによる日本のナンバープレート認識

このプロジェクトは、標準的なCPUからGoogle CoralのようなAIアクセラレーションデバイスまで、さまざまなハードウェアで効率的に動作するように設計された、リアルタイムの日本のナンバープレート認識システムです。カスタムトレーニングされたYOLOモデルを利用し、すべてPythonで書かれています。

## 開発経緯

このプロジェクトは、現在のPythonベースでAIモデル駆動の実装に至るまで、大きな進化を遂げてきました。

最初の概念実証はPythonで書かれましたが、文字認識には`EasyOCR`ライブラリに依存していました。この方法は、特にナンバープレートに見られる複雑な日本の漢字文字に対して、遅すぎて不正確であることが判明しました。

精度を向上させるため、プロジェクトはYOLOアーキテクチャに基づくカスタムトレーニングされたAIモデルに移行しました。最初のモデルがトレーニングおよびテストされ、文字と数字の認識が大幅に改善されました。

アプリケーションの初期バージョンは、Raspberry Pi 4でのパフォーマンスを最大化するためにC++に変換されました。しかし、保守性を向上させ、開発プロセスを簡素化し、最新のPython AIライブラリを活用するために、プロジェクトはその後、純粋なPython実装に戻りました。現在のバージョンでは、高性能とハードウェアアクセラレーションの自動検出を提供する`ultralytics`ライブラリを使用しています。

## システムアーキテクチャ

*   **プログラミング言語:** Python
*   **AIモデル:** カスタムトレーニングされたYOLOモデル
*   **コアライブラリ:** モデルの読み込みと推論のための`ultralytics`
*   **カメラハンドリング:** Raspberry Piでのネイティブで高性能なカメラアクセスのための`picamera2`
*   **ハードウェアアクセラレーション:** `pycoral`ライブラリを介したGoogle Coral Edge TPUのオプションサポート

## 主要なPythonパッケージ

*   **`cv2` (OpenCV):** カメラのRGB出力をモデルが期待するBGR形式に変換するなど、不可欠な画像処理タスクに使用されます。
*   **`picamera2`:** Raspberry Piカメラモジュールを制御するための公式ライブラリで、ビデオキャプチャのための高性能で低オーバーヘッドのインターフェースを提供します。
*   **`numpy`:** 数値演算の基本的なパッケージで、画像データを配列として扱うために広く使用されます。
*   **`ultralytics`:** YOLOオブジェクト検出モデルを読み込んで実行し、推論とハードウェアアクセラレーションを処理するコアフレームワークです。
*   **`PyYAML`:** `config.yaml`ファイルからアプリケーションの設定とモデル構成を読み込むために使用され、コードを変更することなく簡単に調整できます。
*   **`threading`と`queue`:** これらの組み込みライブラリは、AIモデルの推論を別のスレッドで実行するために一緒に使用され、検出結果を待っている間にカメラフィードがカクつくのを防ぎます。

## 技術的根拠

### なぜC++ではなくPythonなのか？

C++は生のパフォーマンスを提供しますが、`ultralytics`のような最新のPython AIライブラリはパフォーマンスの差を大幅に縮めています。Pythonを使用することで、プロジェクトは以下の利点を享受します：
*   **迅速な開発:** Pythonの簡潔な構文と広範なエコシステムにより、より高速なプロトタイピングとイテレーションが可能です。
*   **保守性:** Pythonコードは一般的に、低レベルのC++コードよりも読みやすく、デバッグしやすく、保守しやすいです。
*   **自動ハードウェアアクセラレーション:** `ultralytics`ライブラリは、利用可能なハードウェアアクセラレータ（GoogleのEdge TPUなど）をコードの変更なしで自動的に検出して使用でき、コンパイル言語に匹敵するパフォーマンスを提供します。

### なぜOCRではなくカスタムYOLOモデルなのか？

`EasyOCR`のような標準的な光学文字認識（OCR）ライブラリは汎用であり、ナンバープレート特有のフォント、角度、照明条件に苦労することがよくあります。カスタムトレーニングされたYOLOモデルは、日本のナンバープレートの文字とレイアウトを認識するように特別にトレーニングされているため、優れた精度を提供します。

### なぜカメラハンドリングに`picamera2`なのか？

`picamera2`ライブラリは、Raspberry Piのカメラモジュール用の公式で最新のPythonインターフェースです。古くて非推奨の`picamera`ライブラリを置き換え、`cv2.VideoCapture`のような汎用ソリューションに比べていくつかの利点があります：

*   **公式サポートと安定性:** 公式にサポートされているライブラリとして、`picamera2`はRaspberry Piハードウェアに最適化されており、継続的なアップデートを受けて長期的な安定性を保証します。
*   **高性能:** カメラのハードウェア機能への直接的で低レベルのアクセスを提供し、オーバーヘッドを最小限に抑え、可能な限り最高のパフォーマンスを提供します。
*   **高度な設定:** 露出、ホワイトバランス、フォーカスなどのカメラ設定を微調整するための豊富なコントロールを公開しており、さまざまな光条件で高い検出精度を達成するために重要です。
*   **シームレスな統合:** Raspberry Pi OSやOpenCVなどの他のPythonライブラリとスムーズに統合し、このプラットフォームでのコンピュータビジョンプロジェクトにとって最も信頼性の高い選択肢となります。

### なぜUbuntu ServerではなくRaspberry Pi OS 64-bit Liteなのか？

Ubuntu Serverは有能なオペレーティングシステムですが、このプロジェクトではいくつかの重要な理由からRaspberry Pi OS 64-bit Liteが選択されました：

*   **ハードウェアの最適化と安定性:** 公式OSとして、Raspberry Piのハードウェアに合わせて細かく調整されており、箱から出してすぐに最大限の安定性とパフォーマンスを保証します。
*   **シームレスなカメラ統合:** Raspberry Pi OSは、`picamera2`などのライブラリを介してカメラモジュールをネイティブで十分に文書化されたサポートを提供します。これにより、他のオペレーティングシステムで発生する可能性のある複雑な設定や互換性の問題を回避できます。
*   **保証されたパッケージの可用性:** ハードウェアとの対話のための重要なPythonパッケージ（GPIO制御やカメラアクセスなど）は公式リポジトリで維持されており、簡単なインストールが保証されます。
*   **広範なコミュニティサポート:** 広大で活発なRaspberry Piコミュニティは、OSに特化した豊富なチュートリアル、フォーラム、トラブルシューティングガイドを提供しており、開発と問題解決がはるかに容易になります。
*   **最小限のリソースフットプリント:** 「Lite」バージョンは、デスクトップ環境のないヘッドレスで最小限のインストールです。これにより、CPUとRAMが節約され、より多くのシステムリソースがAI推論タスクに割り当てられます。

### なぜCPUを1.9 GHzにオーバークロックするのか？

Raspberry Pi 4のデフォルトのCPUクロック速度は1.5 GHzです。1.9 GHzにオーバークロックすると、25%以上の大幅なパフォーマンス向上が得られます。これは、推論時間を短縮し、より高いフレームレートを可能にするため、リアルタイムのオブジェクト検出にとって重要です。CPUはTFLiteモデルを実行するための主要なプロセッサであるため、クロック速度が速いほど直接的に検出が速くなります。

**オーバークロックの方法:**
*続行する前に、適切な冷却（ファンやヒートシンクなど）があることを確認してください。*
1.  `/boot/firmware/config.txt`ファイルを編集します：
    ```bash
    sudo nano /boot/config.txt
    ```
2.  ファイルの末尾に次の行を追加します：
    ```
    # Overclock
    over_voltage=4
    arm_freq=1900
    ```
3.  ファイルを保存して再起動します。

## プログラムの仕組み

このアプリケーションは、ナンバープレート検出のためにフレームを処理しながら、スムーズなリアルタイムビデオフィードを確保するためにマルチスレッド設計を使用しています。

1.  **初期化:**
    *   スクリプトはYOLOモデル（CPU用の`.pt`またはEdge TPU用の`.tflite`）を読み込みます。

2.  **ワーカースレッド:**
    *   AI推論を処理するために、別の「ワーカー」スレッドが起動されます。
    *   メインスレッドはカメラからフレームをキャプチャし、`frame_queue`に入れます。
    *   ワーカースレッドはキューからフレームを取得し、YOLOモデルの予測を実行し、結果（バウンディングボックスとクラス名）を`result_queue`に入れます。
    *   この設計により、AIモデルがフレームを処理するのを待っている間にメインスレッドがフリーズするのを防ぎます。

3.  **メインスレッド:**
    *   メインスレッドは、`picamera2`ライブラリを使用してカメラから継続的にビデオをキャプチャします。
    *   ワーカースレッドからの新しい検出結果がないか`result_queue`をチェックします。
    *   結果が利用可能になると、検出された文字を解析し、ナンバープレート文字列にフォーマットして、結果をコンソールに出力します。
    *   重複した読み取りでコンソールがスパムされるのを避けるために、最後に検出されたプレートを追跡します。

4.  **解析と翻訳:**
    *   `get_yolo_parsed_strings`関数は、生の検出結果を整理します。メインの「NumberPLATE」バウンディングボックスを識別して、上段と下段の垂直位置を決定します。
    *   文字は、正しい順序を形成するために水平位置でソートされます。
    *   `detect-v2.py`スクリプトは、`location_dictionary`を使用して、地名を英語から日本語に翻訳します。
    *   `detect.py`と`detect-v2.py`の両方が、中央の`config.yaml`ファイルからすべての設定を読み込むようになりました。これには、モデルパス、カメラ設定、ひらがな検出などの機能フラグが含まれます。

## モデルのトレーニング

YOLOモデルは、Roboflowの[日本のナンバープレートデータセット](https://universe.roboflow.com/moriken/number-plate-in-japan)を使用してGoogle Colabでトレーニングされました。

### Google Colabでのトレーニング手順

1.  **環境のセットアップ:** 新しいGoogle Colabノートブックを開き、ランタイムをGPUアクセラレータ（`ランタイム` -> `ランタイムのタイプを変更` -> `T4`）を使用するように設定します。
2.  **依存関係のインストール:** `ultralytics`ライブラリをインストールします。
    ```python
    !pip install ultralytics
    ```
3.  **モデルのトレーニング:**
    ```python
    !yolo task=detect mode=train model=yolov8n.pt data=./data.yaml epochs=100 imgsz=640
    ```
4.  **TFLiteへのエクスポート（CPU用）:**
    ```python
    !yolo export model=runs/detect/train/weights/best.pt format=tflite
    ```
5.  **Edge TPU TFLiteへのエクスポート（Coral用）:**
    ```bash
    # TFLiteモデルをEdge TPU用にコンパイルするには、Googleの指示に従う必要があります。
    # この手順は通常、Edge TPUコンパイラがインストールされたLinuxマシンで実行されます。
    ```

## ユーティリティ

### カメラフォーカス (`test/focus_camera.py`)

メインの検出スクリプトを実行する前に、カメラが適切にフォーカスされていることを確認することが重要です。ぼやけた画像は検出精度を大幅に低下させます。このスクリプトは、レンズを手動で調整できるように、ライブカメラフィードを表示する簡単な方法を提供します。

**使用方法:**
1.  `test`ディレクトリに移動します。
2.  スクリプトを実行します：
    ```bash
    python focus_camera.py
    ```
3.  カメラフィードを示すウィンドウが表示されます。画像がシャープでクリアになるまで、カメラのレンズを物理的に調整します。
4.  'q'を押してフィードを閉じます。

## 実行方法

### 1. リポジトリのクローン

リポジトリをクローンするためのAPI-TOKENを受け取るには、管理者に連絡してください。
```bash
git clone https://x-bitbucket-api-token-auth:{API-TOKEN}@bitbucket.org/lixilg/ai-vision.git
cd ai-vision
```

### 2. 依存関係のインストール

このプロジェクトにはPythonといくつかのライブラリが必要です。
```bash
# Raspberry Pi以外のシステムでの標準的なCPU使用の場合
pip install ultralytics opencv-python

# Raspberry Piでの使用の場合
pip install ultralytics opencv-python picamera2

# Raspberry PiでのGoogle Coralの使用の場合
pip install ultralytics "opencv-python<4.9" "tflite-runtime>=2.14" pycoral picamera2
```

### 3. アプリケーションの設定 (`config.yaml`)

アプリケーションのすべての設定は`config.yaml`ファイルで管理されます。実行する前に、以下を調整する必要がある場合があります：

*   `model_path`: YOLOモデルファイル（`.pt`）へのパス。
*   `edge_tpu_model_path`: YOLOエッジTPUモデルファイル（`.tflite.pt`）へのパス。
*   `data_path`: トレーニングデータセットの`data.yaml`ファイルへのパス。
*   `use_hiragana`: ひらがな文字の検出とマッピングを有効にするには`true`に設定します。
*   `location_dictionary`: 検出された地名を日本の漢字表現にマッピングします。

### 4. アプリケーションの実行

このプロジェクトには、検出を実行するための2つのスクリプトが含まれています。両方のスクリプトは、`config.yaml`から設定を読み込むようになりました。

#### 標準CPU検出 (`detect.py`)

このスクリプトは、完全な`ultralytics`ライブラリを使用して、コンピュータのCPUでYOLOモデルを実行します。

**実行するには:**
```bash
python detect.py
```

#### Google Coralによる高速化検出 (`detect-v2.py`)

このスクリプトは、[Google Coral USB Accelerator](https://coral.ai/products/accelerator/)での使用に最適化されています。Edge TPUコンパイル済みのTFLiteモデルを使用し、推論をCoralのTPUにオフロードして、大幅なパフォーマンス向上を実現します。

**実行するには:**
1.  Google Coralが接続され、[Edge TPUランタイムがインストールされている](https://coral.ai/docs/accelerator/get-started/)ことを確認します。
2.  スクリプトを実行します：
```bash
python detect-v2.py
```

## 改善の余地

*   **API統合:** 現在のスクリプトはコンソールに出力します。マルチスレッド設計は、検出されたプレートをノンブロッキング方式でWeb APIに送信するように簡単に拡張できます。
*   **モデルの枝刈り:** Edge TPUは高速化を提供しますが、枝刈りのようなさらなるモデル最適化技術は、モデルサイズを縮小し、速度をさらに向上させる可能性があります。
*   **ZRAM統合:** Raspberry Piのようなメモリが限られているデバイスでは、ZRAMを使用すると安定性が向上します。
    *   **簡単な説明:** ZRAMは、デバイスのメモリを実際よりも大きく見せる巧妙な方法と考えてください。使用頻度の低いデータを圧縮してスペースを解放し、プログラムが物理的に利用可能なメモリよりも多くのメモリを必要とする場合にクラッシュするのを防ぎます。
    *   **技術的な説明:** ZRAMは、RAM内に圧縮ブロックデバイスを作成し、スワップディスクとして機能させます。これにより、高負荷時にシステムがメモリ不足になるのを防ぎ、SDカード上の従来のスワップファイルのパフォーマンスコストなしで安定性を向上させることができます。

    **Raspberry Pi OSでZRAMを設定する方法:**

    1.  **ZRAMサービスのインストール:**
        ```bash
        sudo apt update
        sudo apt install zram-tools
        ```

    2.  **ZRAMの設定（オプションですが推奨）:**
        デフォルトでは、`zram-tools`はRAMの50%をZRAMスワップに割り当てます。設定ファイルを編集することでこれを調整できます：
        ```bash
        sudo nano /etc/default/zramswap
        ```
        `ALGO`と`PERCENT`の設定を探します。たとえば、`lz4`アルゴリズム（デフォルトの`lzo`よりも高速）を使用し、RAMの75%を割り当てるには、次のように変更します：
        ```
        ALGO=lz4
        PERCENT=75
        ```

    3.  **システムの再起動:**
        変更を有効にするには、Raspberry Piを再起動します。
        ```bash
        sudo reboot
        ```

    4.  **ZRAMがアクティブであることを確認:**
        再起動後、次のコマンドを実行してZRAMスワップのステータスを確認できます：
        ```bash
        swapon --show
        ```
        `/dev/zram0`のエントリが表示されるはずです。
